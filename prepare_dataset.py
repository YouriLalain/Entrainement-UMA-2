#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
PrÃ©paration du dataset pour l'entraÃ®nement de dÃ©tection d'objets avec Ovis
GÃ©nÃ¨re 3 types de conversations par image
"""

import pandas as pd
import json
import os
from pathlib import Path
from collections import defaultdict
import random
from PIL import Image

# Configuration
CSV_PATH = "dataset/labels_my-project-name_2025-10-19-08-36-47.csv"
IMAGES_DIR = "dataset/images"
OUTPUT_DIR = "dataset/processed"
TRAIN_RATIO = 0.8

# Classes (12 labels)
CLASSES = [
    "fuselage", "cockpit", "derive", "empennage", "aile", "aileron",
    "volet", "reacteur", "nacelle_moteur", "tuyere", "train_atterrissage", "porte"
]

def load_and_group_annotations(csv_path):
    """Charge le CSV et groupe les annotations par image"""
    print(f"ğŸ“‚ Lecture du CSV : {csv_path}")
    df = pd.read_csv(csv_path)
    
    # Groupe par image
    grouped = defaultdict(list)
    for _, row in df.iterrows():
        img_name = row['image_name']
        grouped[img_name].append({
            'label': row['label_name'],
            'bbox': [
                float(row['bbox_x']),
                float(row['bbox_y']),
                float(row['bbox_width']),
                float(row['bbox_height'])
            ],
            'image_width': int(row['image_width']),
            'image_height': int(row['image_height'])
        })
    
    print(f"âœ… {len(grouped)} images avec annotations")
    return grouped

def normalize_bbox(bbox, img_width, img_height):
    """Normalise les bbox en [0, 1]"""
    x, y, w, h = bbox
    return [
        x / img_width,
        y / img_height,
        w / img_width,
        h / img_height
    ]

def bbox_to_str(bbox):
    """Convertit bbox en string lisible"""
    x, y, w, h = bbox
    return f"[{x:.3f}, {y:.3f}, {w:.3f}, {h:.3f}]"

# Templates de questions variÃ©es
DETECTION_QUESTIONS = [
    "DÃ©tecte tous les composants de cet avion",
    "Quels sont les Ã©lÃ©ments visibles sur cet avion ?",
    "Liste tous les composants que tu vois",
    "Identifie toutes les parties de cet avion",
    "Analyse les composants de cet appareil",
    "Quelles parties de l'avion sont visibles ?",
    "Fais l'inventaire des composants visibles",
    "RepÃ¨re tous les Ã©lÃ©ments de cet avion",
]

DESCRIPTION_QUESTIONS = [
    "DÃ©cris cet avion",
    "Que vois-tu sur cette image ?",
    "PrÃ©sente cet appareil",
    "Donne-moi une description de cet avion",
    "Que peux-tu me dire sur cet avion ?",
    "Analyse cette image d'avion",
    "DÃ©cris les caractÃ©ristiques visibles",
    "Fais-moi un rÃ©sumÃ© de ce que tu vois",
]

LOCALIZATION_TEMPLATES = {
    "fuselage": ["OÃ¹ est le fuselage ?", "Localise le fuselage", "Montre-moi le fuselage", "Position du fuselage ?"],
    "cockpit": ["OÃ¹ est le cockpit ?", "Localise le cockpit", "Montre-moi le cockpit", "OÃ¹ se trouve le poste de pilotage ?"],
    "derive": ["OÃ¹ est la dÃ©rive ?", "Localise la dÃ©rive", "Montre-moi la dÃ©rive", "Position de la dÃ©rive ?"],
    "empennage": ["OÃ¹ est l'empennage ?", "Localise l'empennage", "Montre-moi l'empennage", "Position de l'empennage ?"],
    "aile": ["OÃ¹ sont les ailes ?", "Localise les ailes", "Montre-moi les ailes", "Position des ailes ?"],
    "aileron": ["OÃ¹ sont les ailerons ?", "Localise les ailerons", "Montre-moi les ailerons", "Position des ailerons ?"],
    "volet": ["OÃ¹ sont les volets ?", "Localise les volets", "Montre-moi les volets", "Position des volets ?"],
    "reacteur": ["OÃ¹ sont les rÃ©acteurs ?", "Localise les moteurs", "Montre-moi les moteurs", "OÃ¹ sont les moteurs ?", "Position des rÃ©acteurs ?"],
    "nacelle_moteur": ["OÃ¹ sont les nacelles ?", "Localise les nacelles moteur", "Montre-moi les nacelles", "Position des nacelles ?"],
    "tuyere": ["OÃ¹ sont les tuyÃ¨res ?", "Localise les tuyÃ¨res", "Montre-moi les tuyÃ¨res", "Position des tuyÃ¨res ?"],
    "train_atterrissage": ["OÃ¹ est le train d'atterrissage ?", "Localise le train", "Montre-moi le train", "Position du train ?"],
    "porte": ["OÃ¹ sont les portes ?", "Localise les portes", "Montre-moi les portes", "Position des portes ?"],
}

RESPONSE_TEMPLATES = {
    "single": [
        "Le {label} est situÃ© Ã  la position {bbox}",
        "Je vois le {label} Ã  {bbox}",
        "Le {label} se trouve Ã  {bbox}",
        "Voici le {label} : {bbox}",
    ],
    "multiple": [
        "Je dÃ©tecte {count} {label}s aux positions : {bboxes}",
        "Il y a {count} {label}s visibles : {bboxes}",
        "Je vois {count} {label}s : {bboxes}",
        "Voici les {count} {label}s : {bboxes}",
    ]
}

def generate_detection_conversation(image_name, annotations):
    """Type 1 : DÃ©tection exhaustive de tous les composants"""
    components = []
    bboxes = []
    
    for ann in annotations:
        label = ann['label']
        bbox = normalize_bbox(ann['bbox'], ann['image_width'], ann['image_height'])
        components.append(f"{label} Ã  {bbox_to_str(bbox)}")
        bboxes.append({
            'label': label,
            'bbox': bbox
        })
    
    question = random.choice(DETECTION_QUESTIONS)
    response = f"Je dÃ©tecte : {', '.join(components)}"
    
    return {
        'image': image_name,
        'conversations': [
            {'role': 'user', 'content': question},
            {'role': 'assistant', 'content': response}
        ],
        'bboxes': bboxes
    }

def generate_localization_conversation(image_name, annotations):
    """Type 2 : Localisation ciblÃ©e d'un composant spÃ©cifique"""
    # Choisit un composant alÃ©atoire
    ann = random.choice(annotations)
    label = ann['label']
    bbox = normalize_bbox(ann['bbox'], ann['image_width'], ann['image_height'])
    
    # Question variÃ©e
    question = random.choice(LOCALIZATION_TEMPLATES.get(label, [f"OÃ¹ est le {label} ?"]))
    
    # RÃ©ponse variÃ©e
    response = random.choice(RESPONSE_TEMPLATES["single"]).format(
        label=label,
        bbox=bbox_to_str(bbox)
    )
    
    return {
        'image': image_name,
        'conversations': [
            {'role': 'user', 'content': question},
            {'role': 'assistant', 'content': response}
        ],
        'bboxes': [{
            'label': label,
            'bbox': bbox
        }]
    }

def generate_specific_component_conversation(image_name, annotations, target_label):
    """Type 4 : Question sur un composant spÃ©cifique (NOUVEAU - filtre les bboxes)"""
    # Filtre uniquement les annotations du composant ciblÃ©
    filtered_anns = [ann for ann in annotations if ann['label'] == target_label]
    
    if not filtered_anns:
        return None
    
    # Normalise les bboxes
    bboxes = []
    bbox_strings = []
    for ann in filtered_anns:
        bbox = normalize_bbox(ann['bbox'], ann['image_width'], ann['image_height'])
        bboxes.append({
            'label': ann['label'],
            'bbox': bbox
        })
        bbox_strings.append(bbox_to_str(bbox))
    
    # Question variÃ©e
    question = random.choice(LOCALIZATION_TEMPLATES.get(target_label, [f"OÃ¹ sont les {target_label}s ?"]))
    
    # RÃ©ponse selon le nombre
    if len(filtered_anns) == 1:
        response = random.choice(RESPONSE_TEMPLATES["single"]).format(
            label=target_label,
            bbox=bbox_strings[0]
        )
    else:
        response = random.choice(RESPONSE_TEMPLATES["multiple"]).format(
            count=len(filtered_anns),
            label=target_label,
            bboxes=", ".join(bbox_strings)
        )
    
    return {
        'image': image_name,
        'conversations': [
            {'role': 'user', 'content': question},
            {'role': 'assistant', 'content': response}
        ],
        'bboxes': bboxes  # SEULEMENT les bboxes du composant demandÃ©
    }

def generate_description_conversation(image_name, annotations):
    """Type 3 : Description gÃ©nÃ©rale de l'avion"""
    unique_labels = list(set(ann['label'] for ann in annotations))
    num_components = len(annotations)
    
    components_list = ', '.join(unique_labels[:-1]) + f" et {unique_labels[-1]}" if len(unique_labels) > 1 else unique_labels[0]
    
    bboxes = []
    for ann in annotations:
        bbox = normalize_bbox(ann['bbox'], ann['image_width'], ann['image_height'])
        bboxes.append({
            'label': ann['label'],
            'bbox': bbox
        })
    
    # Question variÃ©e
    question = random.choice(DESCRIPTION_QUESTIONS)
    
    # RÃ©ponse variÃ©e
    responses = [
        f"C'est un avion avec {num_components} composants visibles : {components_list}",
        f"Sur cette image, je vois un avion comportant : {components_list}",
        f"Cet appareil prÃ©sente {num_components} Ã©lÃ©ments : {components_list}",
        f"L'avion comporte les composants suivants : {components_list}",
        f"Je peux identifier {num_components} parties : {components_list}",
    ]
    response = random.choice(responses)
    
    return {
        'image': image_name,
        'conversations': [
            {'role': 'user', 'content': question},
            {'role': 'assistant', 'content': response}
        ],
        'bboxes': bboxes
    }

def verify_image_exists(image_name, images_dir):
    """VÃ©rifie que l'image existe"""
    img_path = Path(images_dir) / image_name
    if not img_path.exists():
        return False
    
    try:
        img = Image.open(img_path)
        img.verify()
        return True
    except:
        return False

def create_dataset(annotations_by_image, images_dir):
    """CrÃ©e le dataset avec plusieurs types de conversations (dÃ©cuplÃ©)"""
    dataset = []
    
    print("ğŸ”„ GÃ©nÃ©ration des conversations (mode dÃ©cuplÃ©)...")
    
    for image_name, annotations in annotations_by_image.items():
        # VÃ©rifie que l'image existe
        if not verify_image_exists(image_name, images_dir):
            print(f"âš ï¸  Image non trouvÃ©e : {image_name}, skip")
            continue
        
        # Type 1 : DÃ©tection exhaustive (x2 variations)
        for _ in range(2):
            dataset.append(generate_detection_conversation(image_name, annotations))
        
        # Type 2 : Localisation alÃ©atoire (x3 variations)
        for _ in range(3):
            dataset.append(generate_localization_conversation(image_name, annotations))
        
        # Type 3 : Description gÃ©nÃ©rale (x2 variations)
        for _ in range(2):
            dataset.append(generate_description_conversation(image_name, annotations))
        
        # Type 4 : Questions ciblÃ©es par composant (NOUVEAU - dÃ©cuple le dataset)
        # Pour chaque type de composant prÃ©sent dans l'image
        unique_labels = list(set(ann['label'] for ann in annotations))
        for label in unique_labels:
            # GÃ©nÃ¨re 2 questions par composant prÃ©sent
            for _ in range(2):
                conv = generate_specific_component_conversation(image_name, annotations, label)
                if conv:
                    dataset.append(conv)
    
    print(f"âœ… {len(dataset)} conversations gÃ©nÃ©rÃ©es (dataset dÃ©cuplÃ©)")
    return dataset

def split_dataset(dataset, train_ratio=0.8):
    """Split train/val"""
    random.shuffle(dataset)
    split_idx = int(len(dataset) * train_ratio)
    
    train_data = dataset[:split_idx]
    val_data = dataset[split_idx:]
    
    print(f"ğŸ“Š Split : {len(train_data)} train, {len(val_data)} val")
    return train_data, val_data

def save_dataset(train_data, val_data, output_dir):
    """Sauvegarde les datasets"""
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    train_path = output_path / "train.json"
    val_path = output_path / "val.json"
    
    with open(train_path, 'w', encoding='utf-8') as f:
        json.dump(train_data, f, ensure_ascii=False, indent=2)
    
    with open(val_path, 'w', encoding='utf-8') as f:
        json.dump(val_data, f, ensure_ascii=False, indent=2)
    
    print(f"ğŸ’¾ Dataset sauvegardÃ© dans {output_dir}")
    print(f"   - train.json : {len(train_data)} samples")
    print(f"   - val.json : {len(val_data)} samples")

def main():
    print("="*60)
    print("ğŸš€ PRÃ‰PARATION DU DATASET POUR OVIS DETECTION")
    print("="*60)
    
    # Charge les annotations
    annotations_by_image = load_and_group_annotations(CSV_PATH)
    
    # CrÃ©e le dataset
    dataset = create_dataset(annotations_by_image, IMAGES_DIR)
    
    # Split train/val
    train_data, val_data = split_dataset(dataset, TRAIN_RATIO)
    
    # Sauvegarde
    save_dataset(train_data, val_data, OUTPUT_DIR)
    
    # Statistiques
    print("\nğŸ“ˆ STATISTIQUES")
    print(f"   - Images uniques : {len(annotations_by_image)}")
    print(f"   - Total conversations : {len(dataset)}")
    print(f"   - Classes : {len(CLASSES)}")
    
    print("\nâœ… DATASET PRÃŠT !")

if __name__ == "__main__":
    main()
