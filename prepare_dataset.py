#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Pr√©paration du dataset pour l'entra√Ænement de d√©tection d'objets avec Ovis
G√©n√®re 3 types de conversations par image
"""

import pandas as pd
import json
import os
from pathlib import Path
from collections import defaultdict
import random
from PIL import Image

# Configuration
CSV_PATH = "dataset/labels_my-project-name_2025-10-19-08-36-47.csv"
IMAGES_DIR = "dataset/images"
OUTPUT_DIR = "dataset/processed"
TRAIN_RATIO = 0.8

# Classes (12 labels)
CLASSES = [
    "fuselage", "cockpit", "derive", "empennage", "aile", "aileron",
    "volet", "reacteur", "nacelle_moteur", "tuyere", "train_atterrissage", "porte"
]

def load_and_group_annotations(csv_path):
    """Charge le CSV et groupe les annotations par image"""
    print(f"üìÇ Lecture du CSV : {csv_path}")
    df = pd.read_csv(csv_path)
    
    # Groupe par image
    grouped = defaultdict(list)
    for _, row in df.iterrows():
        img_name = row['image_name']
        grouped[img_name].append({
            'label': row['label_name'],
            'bbox': [
                float(row['bbox_x']),
                float(row['bbox_y']),
                float(row['bbox_width']),
                float(row['bbox_height'])
            ],
            'image_width': int(row['image_width']),
            'image_height': int(row['image_height'])
        })
    
    print(f"‚úÖ {len(grouped)} images avec annotations")
    return grouped

def normalize_bbox(bbox, img_width, img_height):
    """Normalise les bbox en [0, 1]"""
    x, y, w, h = bbox
    return [
        x / img_width,
        y / img_height,
        w / img_width,
        h / img_height
    ]

def bbox_to_str(bbox):
    """Convertit bbox en string lisible"""
    x, y, w, h = bbox
    return f"[{x:.3f}, {y:.3f}, {w:.3f}, {h:.3f}]"

# Templates de questions vari√©es
DETECTION_QUESTIONS = [
    "D√©tecte tous les composants de cet avion",
    "Quels sont les √©l√©ments visibles sur cet avion ?",
    "Liste tous les composants que tu vois",
    "Identifie toutes les parties de cet avion",
    "Analyse les composants de cet appareil",
    "Quelles parties de l'avion sont visibles ?",
    "Fais l'inventaire des composants visibles",
    "Rep√®re tous les √©l√©ments de cet avion",
]

DESCRIPTION_QUESTIONS = [
    "D√©cris cet avion",
    "Que vois-tu sur cette image ?",
    "Pr√©sente cet appareil",
    "Donne-moi une description de cet avion",
    "Que peux-tu me dire sur cet avion ?",
    "Analyse cette image d'avion",
    "D√©cris les caract√©ristiques visibles",
    "Fais-moi un r√©sum√© de ce que tu vois",
]

LOCALIZATION_TEMPLATES = {
    "fuselage": ["O√π est le fuselage ?", "Localise le fuselage", "Montre-moi le fuselage", "Position du fuselage ?"],
    "cockpit": ["O√π est le cockpit ?", "Localise le cockpit", "Montre-moi le cockpit", "O√π se trouve le poste de pilotage ?"],
    "derive": ["O√π est la d√©rive ?", "Localise la d√©rive", "Montre-moi la d√©rive", "Position de la d√©rive ?"],
    "empennage": ["O√π est l'empennage ?", "Localise l'empennage", "Montre-moi l'empennage", "Position de l'empennage ?"],
    "aile": ["O√π sont les ailes ?", "Localise les ailes", "Montre-moi les ailes", "Position des ailes ?"],
    "aileron": ["O√π sont les ailerons ?", "Localise les ailerons", "Montre-moi les ailerons", "Position des ailerons ?"],
    "volet": ["O√π sont les volets ?", "Localise les volets", "Montre-moi les volets", "Position des volets ?"],
    "reacteur": ["O√π sont les r√©acteurs ?", "Localise les moteurs", "Montre-moi les moteurs", "O√π sont les moteurs ?", "Position des r√©acteurs ?"],
    "nacelle_moteur": ["O√π sont les nacelles ?", "Localise les nacelles moteur", "Montre-moi les nacelles", "Position des nacelles ?"],
    "tuyere": ["O√π sont les tuy√®res ?", "Localise les tuy√®res", "Montre-moi les tuy√®res", "Position des tuy√®res ?"],
    "train_atterrissage": ["O√π est le train d'atterrissage ?", "Localise le train", "Montre-moi le train", "Position du train ?"],
    "porte": ["O√π sont les portes ?", "Localise les portes", "Montre-moi les portes", "Position des portes ?"],
}

RESPONSE_TEMPLATES = {
    "single": [
        "Le {label} est situ√© √† la position {bbox}",
        "Je vois le {label} √† {bbox}",
        "Le {label} se trouve √† {bbox}",
        "Voici le {label} : {bbox}",
    ],
    "multiple": [
        "Je d√©tecte {count} {label}s aux positions : {bboxes}",
        "Il y a {count} {label}s visibles : {bboxes}",
        "Je vois {count} {label}s : {bboxes}",
        "Voici les {count} {label}s : {bboxes}",
    ]
}

def generate_detection_conversation(image_name, annotations):
    """Type 1 : D√©tection exhaustive de tous les composants"""
    components = []
    bboxes = []
    
    for ann in annotations:
        label = ann['label']
        bbox = normalize_bbox(ann['bbox'], ann['image_width'], ann['image_height'])
        components.append(f"{label} √† {bbox_to_str(bbox)}")
        bboxes.append({
            'label': label,
            'bbox': bbox
        })
    
    question = random.choice(DETECTION_QUESTIONS)
    response = f"Je d√©tecte : {', '.join(components)}"
    
    return {
        'image': image_name,
        'conversations': [
            {'role': 'user', 'content': question},
            {'role': 'assistant', 'content': response}
        ],
        'bboxes': bboxes
    }

def generate_localization_conversation(image_name, annotations):
    """Type 2 : Localisation cibl√©e d'un composant sp√©cifique"""
    # Choisit un composant al√©atoire
    ann = random.choice(annotations)
    label = ann['label']
    bbox = normalize_bbox(ann['bbox'], ann['image_width'], ann['image_height'])
    
    # Question vari√©e
    question = random.choice(LOCALIZATION_TEMPLATES.get(label, [f"O√π est le {label} ?"]))
    
    # R√©ponse vari√©e
    response = random.choice(RESPONSE_TEMPLATES["single"]).format(
        label=label,
        bbox=bbox_to_str(bbox)
    )
    
    return {
        'image': image_name,
        'conversations': [
            {'role': 'user', 'content': question},
            {'role': 'assistant', 'content': response}
        ],
        'bboxes': [{
            'label': label,
            'bbox': bbox
        }]
    }

def generate_specific_component_conversation(image_name, annotations, target_label):
    """Type 4 : Question sur un composant sp√©cifique (NOUVEAU - filtre les bboxes)"""
    # Filtre uniquement les annotations du composant cibl√©
    filtered_anns = [ann for ann in annotations if ann['label'] == target_label]
    
    if not filtered_anns:
        return None
    
    # Normalise les bboxes
    bboxes = []
    bbox_strings = []
    for ann in filtered_anns:
        bbox = normalize_bbox(ann['bbox'], ann['image_width'], ann['image_height'])
        bboxes.append({
            'label': ann['label'],
            'bbox': bbox
        })
        bbox_strings.append(bbox_to_str(bbox))
    
    # Question vari√©e
    question = random.choice(LOCALIZATION_TEMPLATES.get(target_label, [f"O√π sont les {target_label}s ?"]))
    
    # R√©ponse selon le nombre
    if len(filtered_anns) == 1:
        response = random.choice(RESPONSE_TEMPLATES["single"]).format(
            label=target_label,
            bbox=bbox_strings[0]
        )
    else:
        response = random.choice(RESPONSE_TEMPLATES["multiple"]).format(
            count=len(filtered_anns),
            label=target_label,
            bboxes=", ".join(bbox_strings)
        )
    
    return {
        'image': image_name,
        'conversations': [
            {'role': 'user', 'content': question},
            {'role': 'assistant', 'content': response}
        ],
        'bboxes': bboxes  # SEULEMENT les bboxes du composant demand√©
    }

def generate_description_conversation(image_name, annotations):
    """Type 3 : Description g√©n√©rale de l'avion"""
    unique_labels = list(set(ann['label'] for ann in annotations))
    num_components = len(annotations)
    
    components_list = ', '.join(unique_labels[:-1]) + f" et {unique_labels[-1]}" if len(unique_labels) > 1 else unique_labels[0]
    
    bboxes = []
    for ann in annotations:
        bbox = normalize_bbox(ann['bbox'], ann['image_width'], ann['image_height'])
        bboxes.append({
            'label': ann['label'],
            'bbox': bbox
        })
    
    # Question vari√©e
    question = random.choice(DESCRIPTION_QUESTIONS)
    
    # R√©ponse vari√©e
    responses = [
        f"C'est un avion avec {num_components} composants visibles : {components_list}",
        f"Sur cette image, je vois un avion comportant : {components_list}",
        f"Cet appareil pr√©sente {num_components} √©l√©ments : {components_list}",
        f"L'avion comporte les composants suivants : {components_list}",
        f"Je peux identifier {num_components} parties : {components_list}",
    ]
    response = random.choice(responses)
    
    return {
        'image': image_name,
        'conversations': [
            {'role': 'user', 'content': question},
            {'role': 'assistant', 'content': response}
        ],
        'bboxes': bboxes
    }

def verify_image_exists(image_name, images_dir):
    """V√©rifie que l'image existe"""
    img_path = Path(images_dir) / image_name
    if not img_path.exists():
        return False
    
    try:
        img = Image.open(img_path)
        img.verify()
        return True
    except:
        return False

def create_dataset(annotations_by_image, images_dir):
    """Cr√©e le dataset avec plusieurs types de conversations (d√©cupl√©)"""
    dataset = []
    
    print("üîÑ G√©n√©ration des conversations (mode d√©cupl√©)...")
    
    for image_name, annotations in annotations_by_image.items():
        # V√©rifie que l'image existe
        if not verify_image_exists(image_name, images_dir):
            print(f"‚ö†Ô∏è  Image non trouv√©e : {image_name}, skip")
            continue
        
        # Type 1 : D√©tection exhaustive (x2 variations)
        for _ in range(2):
            dataset.append(generate_detection_conversation(image_name, annotations))
        
        # Type 2 : Localisation al√©atoire (x3 variations)
        for _ in range(3):
            dataset.append(generate_localization_conversation(image_name, annotations))
        
        # Type 3 : Description g√©n√©rale (x2 variations)
        for _ in range(2):
            dataset.append(generate_description_conversation(image_name, annotations))
        
        # Type 4 : Questions cibl√©es par composant (NOUVEAU - d√©cuple le dataset)
        # Pour chaque type de composant pr√©sent dans l'image
        unique_labels = list(set(ann['label'] for ann in annotations))
        for label in unique_labels:
            # G√©n√®re 2 questions par composant pr√©sent
            for _ in range(2):
                conv = generate_specific_component_conversation(image_name, annotations, label)
                if conv:
                    dataset.append(conv)
    
    print(f"‚úÖ {len(dataset)} conversations g√©n√©r√©es (dataset d√©cupl√©)")
    return dataset

def split_dataset(dataset, train_ratio=0.8):
    """Split train/val"""
    random.shuffle(dataset)
    split_idx = int(len(dataset) * train_ratio)
    
    train_data = dataset[:split_idx]
    val_data = dataset[split_idx:]
    
    print(f"üìä Split : {len(train_data)} train, {len(val_data)} val")
    return train_data, val_data

def save_dataset(train_data, val_data, output_dir):
    """Sauvegarde les datasets"""
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    train_path = output_path / "train.json"
    val_path = output_path / "val.json"
    
    with open(train_path, 'w', encoding='utf-8') as f:
        json.dump(train_data, f, ensure_ascii=False, indent=2)
    
    with open(val_path, 'w', encoding='utf-8') as f:
        json.dump(val_data, f, ensure_ascii=False, indent=2)
    
    print(f"üíæ Dataset sauvegard√© dans {output_dir}")
    print(f"   - train.json : {len(train_data)} samples")
    print(f"   - val.json : {len(val_data)} samples")

def main():
    print("="*60)
    print("üöÄ PR√âPARATION DU DATASET POUR OVIS DETECTION")
    print("="*60)
    
    # Charge les annotations
    annotations_by_image = load_and_group_annotations(CSV_PATH)
    
    # Cr√©e le dataset
    dataset = create_dataset(annotations_by_image, IMAGES_DIR)
    
    # Split train/val
    train_data, val_data = split_dataset(dataset, TRAIN_RATIO)
    
    # Sauvegarde
    save_dataset(train_data, val_data, OUTPUT_DIR)
    
    # Statistiques
    print("\nüìà STATISTIQUES")
    print(f"   - Images uniques : {len(annotations_by_image)}")
    print(f"   - Total conversations : {len(dataset)}")
    print(f"   - Classes : {len(CLASSES)}")
    
    print("\n‚úÖ DATASET PR√äT !")

if __name__ == "__main__":
    main()
