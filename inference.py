#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Script d'inf√©rence pour tester le mod√®le entra√Æn√©
"""

import torch
import yaml
from pathlib import Path
from PIL import Image, ImageDraw, ImageFont
import torchvision.transforms as transforms
import argparse

from model import load_model, load_checkpoint


# Classes
CLASSES = [
    "fuselage", "cockpit", "derive", "empennage", "aile", "aileron",
    "volet", "reacteur", "nacelle_moteur", "tuyere", "train_atterrissage", "porte"
]


def load_inference_model(checkpoint_path, config_path='config.yaml'):
    """Charge le mod√®le depuis un checkpoint"""
    
    # Charge la config
    with open(config_path, 'r') as f:
        config = yaml.safe_load(f)
    
    # Cr√©e le mod√®le
    device = torch.device(config['device'])
    model = load_model(config)
    
    # Charge le checkpoint
    load_checkpoint(model, checkpoint_path, device)
    
    model.eval()
    
    return model, device, config


def preprocess_image(image_path, size=(384, 384)):
    """Pr√©process l'image pour l'inf√©rence"""
    
    image = Image.open(image_path).convert('RGB')
    original_size = image.size
    
    transform = transforms.Compose([
        transforms.Resize(size),
        transforms.ToTensor(),
        transforms.Normalize(
            mean=[0.485, 0.456, 0.406],
            std=[0.229, 0.224, 0.225]
        )
    ])
    
    image_tensor = transform(image).unsqueeze(0)
    
    return image_tensor, image, original_size


def postprocess_predictions(cls_scores, bbox_preds, conf_threshold=0.5, nms_threshold=0.5):
    """Post-traite les pr√©dictions"""
    
    # cls_scores: [1, num_classes, H, W]
    # bbox_preds: [1, 4, H, W]
    
    batch_size, num_classes, H, W = cls_scores.shape
    
    # Trouve les d√©tections avec score > threshold
    cls_probs = torch.softmax(cls_scores, dim=1)
    max_probs, max_classes = cls_probs.max(dim=1)  # [1, H, W]
    
    # Seuillage
    mask = max_probs[0] > conf_threshold
    
    if not mask.any():
        return []
    
    # R√©cup√®re les positions
    y_coords, x_coords = torch.where(mask)
    
    detections = []
    for y, x in zip(y_coords, x_coords):
        class_id = max_classes[0, y, x].item()
        confidence = max_probs[0, y, x].item()
        
        # R√©cup√®re la bbox
        bbox = bbox_preds[0, :, y, x].cpu().numpy()  # [4]
        
        detections.append({
            'class_id': class_id,
            'class_name': CLASSES[class_id] if class_id < len(CLASSES) else 'unknown',
            'confidence': confidence,
            'bbox': bbox.tolist()  # [x, y, w, h] normalis√©
        })
    
    # Simple NMS (garde les top-K par classe)
    # Version simplifi√©e
    detections = sorted(detections, key=lambda x: x['confidence'], reverse=True)[:10]
    
    return detections


def visualize_detections(image, detections, output_path=None):
    """Visualise les d√©tections sur l'image"""
    
    draw = ImageDraw.Draw(image)
    width, height = image.size
    
    # Couleurs par classe
    colors = [
        '#FF0000', '#00FF00', '#0000FF', '#FFFF00', '#FF00FF', '#00FFFF',
        '#800000', '#008000', '#000080', '#808000', '#800080', '#008080'
    ]
    
    for det in detections:
        x, y, w, h = det['bbox']
        
        # D√©normalise
        x1 = int(x * width)
        y1 = int(y * height)
        x2 = int((x + w) * width)
        y2 = int((y + h) * height)
        
        # Couleur
        color = colors[det['class_id'] % len(colors)]
        
        # Draw bbox
        draw.rectangle([x1, y1, x2, y2], outline=color, width=3)
        
        # Draw label
        label = f"{det['class_name']} {det['confidence']:.2f}"
        
        # Background pour le texte
        try:
            font = ImageFont.truetype("/System/Library/Fonts/Helvetica.ttc", 16)
        except:
            font = ImageFont.load_default()
        
        bbox_text = draw.textbbox((x1, y1), label, font=font)
        draw.rectangle(bbox_text, fill=color)
        draw.text((x1, y1), label, fill='white', font=font)
    
    if output_path:
        image.save(output_path)
        print(f"üíæ Image sauvegard√©e : {output_path}")
    
    return image


def generate_description(detections):
    """G√©n√®re une description textuelle des d√©tections"""
    
    if not detections:
        return "Aucun composant d√©tect√©."
    
    # Groupe par classe
    class_counts = {}
    for det in detections:
        class_name = det['class_name']
        class_counts[class_name] = class_counts.get(class_name, 0) + 1
    
    # G√©n√®re la description
    components = []
    for class_name, count in class_counts.items():
        if count == 1:
            components.append(class_name)
        else:
            components.append(f"{count} {class_name}s")
    
    if len(components) == 1:
        desc = f"Je d√©tecte 1 composant : {components[0]}"
    else:
        desc = f"Je d√©tecte {len(detections)} composants : {', '.join(components[:-1])} et {components[-1]}"
    
    return desc


def inference(image_path, checkpoint_path, config_path='config.yaml', output_path=None, conf_threshold=0.5):
    """Inf√©rence sur une image"""
    
    print("="*70)
    print("üîç INFERENCE OVIS DETECTION")
    print("="*70)
    
    # Charge le mod√®le
    print("üîÑ Chargement du mod√®le...")
    model, device, config = load_inference_model(checkpoint_path, config_path)
    print("‚úÖ Mod√®le charg√©")
    
    # Charge et pr√©process l'image
    print(f"üì∑ Chargement de l'image : {image_path}")
    image_tensor, original_image, original_size = preprocess_image(image_path)
    image_tensor = image_tensor.to(device)
    
    # Inf√©rence
    print("üöÄ Inf√©rence...")
    with torch.no_grad():
        outputs = model(image_tensor)
        cls_scores = outputs['cls_scores']
        bbox_preds = outputs['bbox_preds']
    
    # Post-traitement
    print("üîß Post-traitement...")
    detections = postprocess_predictions(cls_scores, bbox_preds, conf_threshold)
    
    # Affiche les r√©sultats
    print("\n" + "="*70)
    print("üìä R√âSULTATS")
    print("="*70)
    
    if not detections:
        print("‚ùå Aucune d√©tection")
    else:
        print(f"‚úÖ {len(detections)} d√©tection(s)\n")
        for i, det in enumerate(detections, 1):
            print(f"{i}. {det['class_name']}")
            print(f"   - Confiance: {det['confidence']:.3f}")
            print(f"   - BBox: {det['bbox']}\n")
    
    # G√©n√®re la description
    description = generate_description(detections)
    print("üí¨ DESCRIPTION")
    print(f"   {description}\n")
    
    # Visualise
    if output_path or detections:
        print("üé® Visualisation...")
        vis_image = original_image.copy()
        vis_image = visualize_detections(vis_image, detections, output_path)
        
        if not output_path:
            vis_image.show()
    
    print("="*70)
    print("‚úÖ TERMIN√â")
    print("="*70)
    
    return detections, description


def main():
    parser = argparse.ArgumentParser(description='Inf√©rence OVIS Detection')
    parser.add_argument('--image', type=str, required=True, help='Chemin vers l\'image')
    parser.add_argument('--checkpoint', type=str, required=True, help='Chemin vers le checkpoint')
    parser.add_argument('--config', type=str, default='config.yaml', help='Chemin vers la config')
    parser.add_argument('--output', type=str, default=None, help='Chemin de sortie pour l\'image avec d√©tections')
    parser.add_argument('--conf-threshold', type=float, default=0.5, help='Seuil de confiance')
    
    args = parser.parse_args()
    
    inference(
        args.image,
        args.checkpoint,
        args.config,
        args.output,
        args.conf_threshold
    )


if __name__ == "__main__":
    main()
